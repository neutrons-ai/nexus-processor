#!/usr/bin/env python3
"""
Initialize Iceberg Tables for NeXus Data

This script creates the Iceberg table definitions for storing processed
NeXus data in a data lakehouse. It can be run with PySpark or adapted
for other Iceberg-compatible engines (Trino, Flink, etc.).

The tables are partitioned by (instrument_id, run_number) for efficient
queries by instrument or specific runs.

Tables created:
- experiment_runs: Aggregated run metadata with nested sample/instrument info
- daslogs: Time-series DAS log data
- events: Neutron detector events (can be very large)
- event_summary: Summary statistics per detector bank

Usage:
    # With PySpark
    spark-submit init_iceberg_tables.py --catalog my_catalog --database neutron_data
    
    # Generate SQL only (for other engines)
    python init_iceberg_tables.py --sql-only --output create_tables.sql
"""

import argparse
import sys
from pathlib import Path

try:
    from nexus_processor.schemas import (
        EXPERIMENT_RUNS_SCHEMA,
        DASLOGS_SCHEMA,
        EVENTS_SCHEMA,
        EVENT_SUMMARY_SCHEMA,
        get_fields_without_partition,
    )
except ImportError:
    print("ERROR: nexus_processor package not found.")
    print("Install it with: pip install -e /path/to/nexus-processor")
    sys.exit(1)


def pyarrow_type_to_iceberg_sql(pa_type) -> str:
    """Convert PyArrow type to Iceberg SQL type."""
    import pyarrow as pa
    
    type_map = {
        pa.large_string(): 'STRING',
        pa.string(): 'STRING',
        pa.int64(): 'BIGINT',
        pa.int32(): 'INT',
        pa.float64(): 'DOUBLE',
        pa.float32(): 'FLOAT',
        pa.bool_(): 'BOOLEAN',
    }
    
    if pa_type in type_map:
        return type_map[pa_type]
    
    if pa.types.is_map(pa_type):
        key_type = pyarrow_type_to_iceberg_sql(pa_type.key_type)
        value_type = pyarrow_type_to_iceberg_sql(pa_type.item_type)
        return f'MAP<{key_type}, {value_type}>'
    
    if pa.types.is_list(pa_type):
        item_type = pyarrow_type_to_iceberg_sql(pa_type.value_type)
        return f'ARRAY<{item_type}>'
    
    if pa.types.is_struct(pa_type):
        fields = []
        for i in range(pa_type.num_fields):
            field = pa_type.field(i)
            field_type = pyarrow_type_to_iceberg_sql(field.type)
            fields.append(f'{field.name}: {field_type}')
        return f"STRUCT<{', '.join(fields)}>"
    
    return str(pa_type)


def schema_to_sql_columns(schema) -> str:
    """Convert PyArrow schema to SQL column definitions."""
    lines = []
    for field in schema:
        sql_type = pyarrow_type_to_iceberg_sql(field.type)
        comment = ""
        if field.metadata and b'description' in field.metadata:
            desc = field.metadata[b'description'].decode('utf-8')
            comment = f" COMMENT '{desc}'"
        lines.append(f"    {field.name} {sql_type}{comment}")
    return ',\n'.join(lines)


def generate_create_table_sql(
    table_name: str,
    schema,
    catalog: str = "spark_catalog",
    database: str = "neutron_data",
    location: str = None,
) -> str:
    """Generate CREATE TABLE SQL for Iceberg."""
    
    columns = schema_to_sql_columns(schema)
    
    full_table = f"{catalog}.{database}.{table_name}"
    
    sql = f"""CREATE TABLE IF NOT EXISTS {full_table} (
{columns}
)
USING iceberg
PARTITIONED BY (instrument_id, run_number)"""
    
    if location:
        sql += f"\nLOCATION '{location}/{table_name}'"
    
    sql += """
TBLPROPERTIES (
    'write.format.default' = 'parquet',
    'write.parquet.compression-codec' = 'zstd'
)"""
    
    return sql


def generate_all_tables_sql(catalog: str, database: str, location: str = None) -> str:
    """Generate SQL for all tables."""
    
    tables = [
        ("experiment_runs", EXPERIMENT_RUNS_SCHEMA, "Aggregated run metadata with nested sample/instrument info"),
        ("daslogs", DASLOGS_SCHEMA, "Time-series DAS log data from experiments"),
        ("events", EVENTS_SCHEMA, "Neutron detector events (partitioned for large datasets)"),
        ("event_summary", EVENT_SUMMARY_SCHEMA, "Summary statistics per detector bank per run"),
    ]
    
    sql_parts = [
        f"-- Iceberg Tables for NeXus Data",
        f"-- Catalog: {catalog}",
        f"-- Database: {database}",
        f"-- Generated by nexus-processor",
        "",
        f"CREATE DATABASE IF NOT EXISTS {catalog}.{database};",
        "",
    ]
    
    for table_name, schema, description in tables:
        sql_parts.append(f"-- {description}")
        sql_parts.append(generate_create_table_sql(
            table_name, schema, catalog, database, location
        ))
        sql_parts.append(";")
        sql_parts.append("")
    
    return '\n'.join(sql_parts)


def create_tables_with_spark(catalog: str, database: str, location: str = None):
    """Create tables using PySpark."""
    try:
        from pyspark.sql import SparkSession
    except ImportError:
        print("ERROR: PySpark not installed. Use --sql-only to generate SQL.")
        sys.exit(1)
    
    # Initialize Spark with Iceberg
    spark = SparkSession.builder \
        .appName("nexus-iceberg-init") \
        .config("spark.sql.extensions", "org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions") \
        .config(f"spark.sql.catalog.{catalog}", "org.apache.iceberg.spark.SparkCatalog") \
        .getOrCreate()
    
    print(f"Creating database {catalog}.{database}...")
    spark.sql(f"CREATE DATABASE IF NOT EXISTS {catalog}.{database}")
    
    tables = [
        ("experiment_runs", EXPERIMENT_RUNS_SCHEMA),
        ("daslogs", DASLOGS_SCHEMA),
        ("events", EVENTS_SCHEMA),
        ("event_summary", EVENT_SUMMARY_SCHEMA),
    ]
    
    for table_name, schema in tables:
        print(f"Creating table {table_name}...")
        sql = generate_create_table_sql(table_name, schema, catalog, database, location)
        spark.sql(sql)
        print(f"  Created: {catalog}.{database}.{table_name}")
    
    spark.stop()
    print("\nAll tables created successfully!")


def main():
    parser = argparse.ArgumentParser(
        description="Initialize Iceberg tables for NeXus data",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog=__doc__
    )
    
    parser.add_argument(
        "--catalog",
        default="spark_catalog",
        help="Iceberg catalog name (default: spark_catalog)"
    )
    
    parser.add_argument(
        "--database",
        default="neutron_data",
        help="Database name (default: neutron_data)"
    )
    
    parser.add_argument(
        "--location",
        help="Base storage location for tables (e.g., s3://bucket/path)"
    )
    
    parser.add_argument(
        "--sql-only",
        action="store_true",
        help="Generate SQL only, don't execute"
    )
    
    parser.add_argument(
        "--output", "-o",
        help="Output file for SQL (default: stdout)"
    )
    
    args = parser.parse_args()
    
    if args.sql_only:
        sql = generate_all_tables_sql(args.catalog, args.database, args.location)
        
        if args.output:
            Path(args.output).write_text(sql)
            print(f"SQL written to: {args.output}")
        else:
            print(sql)
    else:
        create_tables_with_spark(args.catalog, args.database, args.location)


if __name__ == "__main__":
    main()
