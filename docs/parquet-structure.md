# Parquet File Structure

This document describes the structure of the Parquet files generated by `nexus-processor` when converting NeXus HDF5 files.

## Iceberg Compatibility

All parquet files are generated with **Apache Iceberg-compatible schemas**:

- **Explicit PyArrow schemas** - Consistent column types across files
- **Partition keys** - `run_number` on every file for efficient querying
- **Nested types** - Struct and list types instead of string serialization  
- **Schema metadata** - Field descriptions embedded in parquet metadata
- **Nullable columns** - Graceful handling of missing data

## Output Modes

The converter supports two output modes:

1. **Split files** (default): Creates separate parquet files for each data category
2. **Single file** (`--single-file`): Creates one combined parquet file with all data

---

## Split Files Mode (Default)

When using the default mode, the following files are created:

### `{run}_metadata.parquet`

Run-level metadata with one row per file.

| Column | Type | Description |
|--------|------|-------------|
| `run_number` | int64 | Run number (partition key) |
| `title` | large_string | Experiment title |
| `start_time` | large_string | ISO format start time |
| `end_time` | large_string | ISO format end time |
| `duration` | float64 | Run duration in seconds |
| `proton_charge` | float64 | Total proton charge |
| `total_counts` | int64 | Total neutron counts |
| `experiment_identifier` | large_string | Experiment ID (e.g., IPTS number) |
| `definition` | large_string | NeXus definition name |
| `source_file` | large_string | Original filename |
| `source_path` | large_string | Original file path |
| `ingestion_time` | large_string | ISO format conversion time |
| `file_attributes` | map<string, string> | HDF5 file-level attributes |
| `entry_attributes` | map<string, string> | HDF5 entry-level attributes |

### `{run}_sample.parquet`

Sample information with one row per file.

| Column | Type | Description |
|--------|------|-------------|
| `run_number` | int64 | Run number (partition key) |
| `name` | large_string | Sample name |
| `nature` | large_string | Sample type/nature |
| `chemical_formula` | large_string | Chemical formula |
| `mass` | float64 | Sample mass |
| `temperature` | float64 | Sample temperature |
| `additional_fields` | map<string, string> | Additional sample fields |

### `{run}_instrument.parquet`

Instrument configuration with one row per file.

| Column | Type | Description |
|--------|------|-------------|
| `run_number` | int64 | Run number (partition key) |
| `name` | large_string | Instrument name (e.g., REF_L) |
| `beamline` | large_string | Beamline identifier |
| `instrument_xml_data` | large_string | Instrument definition XML |
| `additional_fields` | map<string, string> | Additional instrument fields |

### `{run}_software.parquet`

Software/provenance information with one row per software component.

| Column | Type | Description |
|--------|------|-------------|
| `run_number` | int64 | Run number (partition key) |
| `component` | large_string | Software component name |
| `name` | large_string | Software name |
| `version` | large_string | Software version |
| `additional_fields` | map<string, string> | Additional software metadata |

### `{run}_users.parquet` (optional, requires `--include-users`)

User/experimenter information with one row per user.

| Column | Type | Description |
|--------|------|-------------|
| `run_number` | int64 | Run number (partition key) |
| `user_id` | large_string | User group identifier (user1, user2, etc.) |
| `name` | large_string | User's full name |
| `facility_user_id` | large_string | Facility user ID |
| `role` | large_string | User's role in the experiment |
| `additional_fields` | map<string, string> | Additional user metadata |

### `{run}_daslogs.parquet`

Data Acquisition System time series logs with one row per time point per log.

| Column | Type | Description |
|--------|------|-------------|
| `run_number` | int64 | Run number (partition key) |
| `log_name` | large_string | Name of the DAS log |
| `device_name` | large_string | Device name |
| `device_id` | large_string | Device identifier |
| `time` | float64 | Time offset in seconds from run start |
| `value` | large_string | Log value (string-encoded) |
| `value_numeric` | float64 | Numeric value if parseable (NULL otherwise) |
| `average_value` | float64 | Average value over the run |
| `min_value` | float64 | Minimum value over the run |
| `max_value` | float64 | Maximum value over the run |

### `{run}_{bank}_events.parquet` (optional, requires `--include-events`)

Neutron detector event data with one row per detected neutron. One file per detector bank.

| Column | Type | Description |
|--------|------|-------------|
| `run_number` | int64 | Run number (partition key) |
| `bank` | large_string | Detector bank name (e.g., "bank1_events") |
| `event_idx` | int64 | Event index within the bank |
| `pulse_index` | int64 | Pulse index (correlates to proton_charge daslog) |
| `event_id` | int64 | Detector pixel ID |
| `time_offset` | float64 | Time offset within pulse (microseconds) |

### `{run}_event_summary.parquet` (optional, requires `--include-events`)

Summary of event data per detector bank.

| Column | Type | Description |
|--------|------|-------------|
| `run_number` | int64 | Run number (partition key) |
| `bank` | large_string | Detector bank name |
| `total_counts` | int64 | Total counts in the bank |
| `n_pulses` | int64 | Number of neutron pulses |
| `events_extracted` | int64 | Number of events extracted |

---

## Single File Mode (`--single-file`)

When using `--single-file`, all data is combined into a single parquet file:

### `{run}_combined.parquet`

Combined file with all data. Each row is either a DAS log entry or an event.

#### Partition/Discrimination Columns

| Column | Type | Description |
|--------|------|-------------|
| `run_number` | int64 | Run number (partition key) |
| `record_type` | large_string | Type of record: `"daslog"` or `"event"` |

#### DAS Log Columns (NULL for events)

| Column | Type | Description |
|--------|------|-------------|
| `log_name` | large_string | Name of the DAS log |
| `device_name` | large_string | Device name |
| `device_id` | large_string | Device identifier |
| `time` | float64 | Time offset in seconds |
| `value` | large_string | Log value |
| `value_numeric` | float64 | Numeric value if parseable |
| `average_value` | float64 | Average value |
| `min_value` | float64 | Minimum value |
| `max_value` | float64 | Maximum value |

#### Event Columns (NULL for daslogs)

| Column | Type | Description |
|--------|------|-------------|
| `bank` | large_string | Detector bank name |
| `event_idx` | int64 | Event index |
| `pulse_index` | int64 | Pulse index (correlates to proton_charge daslog) |
| `event_id` | int64 | Detector pixel ID |
| `time_offset` | float64 | Time offset (microseconds) |

#### Nested Metadata Columns (present on all rows)

Metadata is stored as nested struct types for Iceberg compatibility:

| Column | Type | Description |
|--------|------|-------------|
| `metadata` | struct | Run metadata (see fields below) |
| `sample` | struct | Sample information |
| `instrument` | struct | Instrument information |
| `users` | list<struct> | List of user structs |

**`metadata` struct fields:**
- `title`, `start_time`, `end_time`, `duration`, `proton_charge`, `total_counts`, `experiment_identifier`, `definition`, `source_file`

**`sample` struct fields:**
- `name`, `nature`, `chemical_formula`, `mass`, `temperature`

**`instrument` struct fields:**
- `name`, `beamline`

**`users` list element fields:**
- `name`, `role`

---

## Querying the Combined File

When working with the combined file, use nested column access:

```python
import pandas as pd

df = pd.read_parquet("run_combined.parquet")

# Filter by record_type
daslogs = df[df["record_type"] == "daslog"]
events = df[df["record_type"] == "event"]

# Access nested metadata (pandas automatically unpacks structs)
metadata = df["metadata"].iloc[0]
print(f"Title: {metadata['title']}")
print(f"Duration: {metadata['duration']}s")

# Access sample info
sample = df["sample"].iloc[0]
print(f"Sample: {sample['name']}")

# Access user list
users = df["users"].iloc[0]
for user in users:
    print(f"User: {user['name']} ({user['role']})")

# Use value_numeric for numeric queries (avoids string parsing)
numeric_daslogs = daslogs[daslogs["value_numeric"].notna()]
avg_temp = numeric_daslogs[numeric_daslogs["log_name"] == "temperature"]["value_numeric"].mean()
```

### Iceberg/Spark Queries

```sql
-- Query daslogs with run_number partition pruning
SELECT log_name, value_numeric, metadata.title
FROM nexus_combined
WHERE run_number = 12345
  AND record_type = 'daslog'
  AND log_name = 'temperature';

-- Access nested fields
SELECT 
  metadata.title,
  sample.name AS sample_name,
  instrument.name AS instrument_name
FROM nexus_combined
WHERE run_number = 12345
LIMIT 1;
```

---

## Extracting Events by Time Window

The `pulse_index` column in event data correlates to the `proton_charge` daslog, which records the wall-clock time of each neutron pulse. This enables time-based event filtering.

Use the `scripts/extract_events_by_time.py` script:

```bash
# Extract events in 60-second intervals
python scripts/extract_events_by_time.py data_combined.parquet --interval 60

# Extract events from a specific time range (30-90 seconds)
python scripts/extract_events_by_time.py data_combined.parquet --start 30 --end 90

# Summary only (no file output)
python scripts/extract_events_by_time.py data_combined.parquet --interval 60 --summary

# Filter to a specific detector bank
python scripts/extract_events_by_time.py data_combined.parquet --interval 60 --bank bank1_events
```

The script:
1. Reads pulse times from the `proton_charge` daslog
2. Correlates each event's `pulse_index` to its pulse time
3. Calculates absolute time: `pulse_time + time_offset/1e6`
4. Filters/labels events by time window

---

## Example Usage

```bash
# Default: separate files, no events, no users
nexus-processor input.nxs.h5 -o ./output

# Include events and users
nexus-processor input.nxs.h5 -o ./output --include-events --include-users

# Single combined file with everything
nexus-processor input.nxs.h5 -o ./output --single-file --include-events --include-users

# Limit events for large files
nexus-processor input.nxs.h5 -o ./output --include-events --max-events 100000
```
